{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import warnings \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random \n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as torch_utils\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pack_sequence, pad_packed_sequence, unpack_sequence, PackedSequence, pad_sequence\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import MaxAbsScaler, RobustScaler, Normalizer, QuantileTransformer, PowerTransformer, MinMaxScaler\n",
    "import torch.nn.functional as F\n",
    "import d2l\n",
    "import time\n",
    "import traceback\n",
    "import fastprogress\n",
    "from torchmetrics.classification import BinaryAccuracy, Accuracy \n",
    "import torch.nn.init as init\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from itertools import repeat\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import datetime\n",
    "import h5py\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import importlib\n",
    "from scipy import stats\n",
    "from time import perf_counter, sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearReadout(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple linear readout layer for mapping input features to a single output.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): The size of the input features.\n",
    "\n",
    "    Attributes:\n",
    "        linear (nn.Linear): A fully connected linear layer that maps the input features to a single output.\n",
    "\n",
    "    Methods:\n",
    "        forward(x):\n",
    "            Applies the linear transformation to the input and squeezes the output along the third dimension.\n",
    "\n",
    "    Forward Args:\n",
    "        x (torch.Tensor): A tensor of shape (batch_size, seq_len, input_size) representing the input features.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (batch_size, seq_len) representing the output after applying the linear transformation.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x.squeeze(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonlinearDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network module that decodes hidden representations into output predictions using a linear layer.\n",
    "\n",
    "    Args:\n",
    "        hidden_size (int): The size of the input hidden representations.\n",
    "        output_size (int): The size of the output predictions.\n",
    "        dropout (float): Dropout probability for regularization. Default is 0.0 (no dropout).\n",
    "        device (torch.device, optional): The device to which the module is assigned. Default is None.\n",
    "\n",
    "    Attributes:\n",
    "        linear (nn.Linear): A fully connected linear layer that maps hidden representations to output predictions.\n",
    "        dropout (nn.Dropout): A dropout layer for regularization.\n",
    "\n",
    "    Methods:\n",
    "        forward(factors):\n",
    "            Processes a batch of hidden representations through the linear layer and returns the output predictions.\n",
    "\n",
    "    Forward Args:\n",
    "        factors (list of torch.Tensor): A list of tensors, where each tensor represents a sequence of hidden representations.\n",
    "\n",
    "    Returns:\n",
    "        list of torch.Tensor: A list of tensors, where each tensor represents the decoded output predictions for the corresponding input sequence.\n",
    "\n",
    "    Notes:\n",
    "        - The input `factors` is a list of tensors with varying sequence lengths.\n",
    "        - The method concatenates all sequences, processes them through the linear layer, and splits them back into their original sequence lengths.\n",
    "        - Dropout is currently commented out but can be enabled for regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size=32, output_size=192, dropout=0.0, device=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, factors):\n",
    "        # concatenate all sequences, forward into linear layer and split again\n",
    "        lengths = [factor.shape[0] for factor in factors]\n",
    "        cat_fs = torch.cat(factors, dim=0)\n",
    "      #  cat_fs = self.dropout(cat_fs)\n",
    "        rates = self.linear(cat_fs)\n",
    "        rates = [rate for rate in torch.split(rates, lengths)]\n",
    "        \n",
    "        return rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoreAndReadout(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch module that combines a core encoder and a readout decoder for sequential data processing.\n",
    "\n",
    "    Args:\n",
    "        core (nn.Module): The core module, typically an encoder, that processes the input data.\n",
    "        readout (nn.Module): The readout module, typically a decoder, that generates the final output.\n",
    "\n",
    "    Attributes:\n",
    "        core (nn.Module): The core encoder module.\n",
    "        readout (nn.Module): The readout decoder module.\n",
    "\n",
    "    Methods:\n",
    "        forward(x):\n",
    "            Passes the input through the core module and then through the readout module.\n",
    "\n",
    "    Forward Args:\n",
    "        x (torch.Tensor): The input tensor to be processed by the core module.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The output tensor after processing through the core and readout modules.\n",
    "\n",
    "    Notes:\n",
    "        - This class is designed to separate the encoding (core) and decoding (readout) stages of a model.\n",
    "        - The `core` and `readout` modules can be any PyTorch `nn.Module` that fits the input/output requirements.\n",
    "    \"\"\"\n",
    "    def __init__(self, core, readout):\n",
    "        super().__init__()\n",
    "        self.core = core\n",
    "        self.readout = readout\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.core(x)\n",
    "        x = self.readout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device(cpu_preference=False, verbose=True):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not cpu_preference else \"cpu\")\n",
    "    if verbose: print(\"Using device:\", device)\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_packed_sequences(batch):\n",
    "    \"\"\"\n",
    "    Collates a batch of sequences into a packed sequence for efficient processing in RNNs.\n",
    "\n",
    "    Args:\n",
    "        batch (list of tuples): A batch of data where each element is a tuple (X, Y).\n",
    "            - X: A sequence (e.g., list or array) representing input features.\n",
    "            - Y: A sequence (e.g., list or array) representing target labels.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - PackedSequence: A packed representation of the input sequences (X) for use with PyTorch RNNs.\n",
    "            - list: A list of target sequences (Y) converted to tensors and moved to the specified device.\n",
    "\n",
    "    Notes:\n",
    "        - The function converts input and target sequences to PyTorch tensors and moves them to the device.\n",
    "        - The `pack_sequence` function is used to create a packed sequence from the input data, which is useful for variable-length sequences.\n",
    "        - The `enforce_sorted=False` argument ensures that the sequences do not need to be sorted by length beforehand.\n",
    "    \"\"\"\n",
    "    X, Y = zip(*batch)\n",
    "    X = [torch.Tensor(elem).to(device) for elem in X]\n",
    "    Y = [torch.Tensor(elem).to(device) for elem in Y]\n",
    "    return pack_sequence(X, enforce_sorted=False), Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrialDataset_seq(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for handling sequential trial-based data with optional moving average smoothing.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): A DataFrame containing the trial data.\n",
    "        X (list of str): A list of column names representing the input features.\n",
    "        y (list of str): A list of column names representing the target labels.\n",
    "        seq_len (int): The size of the moving average window for smoothing. Default is 1 (no smoothing).\n",
    "\n",
    "    Attributes:\n",
    "        df (pd.DataFrame): The input DataFrame containing trial data.\n",
    "        X (list of str): The input feature column names.\n",
    "        y (list of str): The target label column names.\n",
    "        bin_size (int): The size of the moving average window.\n",
    "        trials (np.ndarray): An array of unique trial identifiers.\n",
    "\n",
    "    Methods:\n",
    "        __len__():\n",
    "            Returns the number of unique trials in the dataset.\n",
    "\n",
    "        __getitem__(idx):\n",
    "            Retrieves the input features and target labels for a specific trial, applying moving average smoothing.\n",
    "\n",
    "    Forward Args:\n",
    "        idx (int): The index of the trial to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - torch.Tensor: A tensor of smoothed input features for the specified trial.\n",
    "            - torch.Tensor: A tensor of smoothed target labels for the specified trial.\n",
    "\n",
    "    Notes:\n",
    "        - The moving average is applied to both input features and target labels using the specified `seq_len`.\n",
    "        - The `rolling` method is used to compute the moving average, and rows with NaN values are dropped.\n",
    "        - The dataset is designed to handle trial-based data, where each trial is treated as a separate sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, X, y, seq_len=1):\n",
    "        self.df = df\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.bin_size = seq_len\n",
    "        self.trials = df['trial'].unique()  # Unique trial identifiers\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.trials)  # Number of trials\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get all rows for a specific trial\n",
    "        trial_number = self.trials[idx]\n",
    "        trial_data = self.df[self.df['trial'] == trial_number]\n",
    "        \n",
    "        # Compute the moving average for the features\n",
    "        features = trial_data[self.X].rolling(window=self.bin_size).mean().dropna().values\n",
    "        features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "        \n",
    "        # Compute the moving average for the labels (if needed)\n",
    "        labels = trial_data[self.y].rolling(window=self.bin_size).mean().dropna().values\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "        \n",
    "        return features_tensor, labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A GRU-based decoder for processing sequential data and generating output predictions.\n",
    "\n",
    "    Args:\n",
    "        hidden_size (int): The size of the hidden state in the GRU layer. Default is 32.\n",
    "        output_size (int): The size of the output predictions. Default is 192.\n",
    "        num_layers (int): The number of stacked GRU layers. Default is 1.\n",
    "        dropout (float): Dropout probability for regularization. Default is 0.0 (no dropout).\n",
    "        device (torch.device, optional): The device to which the module is assigned. Default is None.\n",
    "\n",
    "    Attributes:\n",
    "        hidden_size (int): The size of the GRU hidden state.\n",
    "        output_size (int): The size of the output predictions.\n",
    "        num_layers (int): The number of GRU layers.\n",
    "        device (torch.device): The device used for computations.\n",
    "        gru (nn.GRU): A GRU layer for processing sequential data.\n",
    "        fc (nn.Linear): A fully connected layer to map GRU outputs to the desired output size.\n",
    "        dropout (nn.Dropout): A dropout layer for regularization.\n",
    "\n",
    "    Methods:\n",
    "        forward(factors):\n",
    "            Processes a list of input sequences through the GRU and generates output predictions.\n",
    "\n",
    "    Forward Args:\n",
    "        factors (list of torch.Tensor): A list of input sequences, where each sequence is a tensor of shape \n",
    "            (sequence_length, hidden_size).\n",
    "\n",
    "    Returns:\n",
    "        list of torch.Tensor: A list of output sequences, where each sequence is a tensor of shape \n",
    "            (sequence_length, output_size).\n",
    "\n",
    "    Notes:\n",
    "        - The input `factors` must be a list of tensors, each representing a sequence.\n",
    "        - The method concatenates all sequences, processes them through the GRU, and splits them back into their original lengths.\n",
    "        - Dropout is applied to the GRU outputs before the linear transformation.\n",
    "        - The GRU operates on sequences in a batch-first format, where the batch size is the total number of sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size=32, output_size=192, num_layers=1, dropout=0.0, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "\n",
    "        # Define the GRU layer\n",
    "        self.gru = nn.GRU(input_size=hidden_size, \n",
    "                          hidden_size=hidden_size, \n",
    "                          num_layers=num_layers, \n",
    "                          batch_first=True, \n",
    "                          dropout=dropout)\n",
    "\n",
    "        # Output linear layer to transform GRU outputs to desired output size\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        # Optional dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, factors):\n",
    "        # Ensure the input is a list of sequences (tensors)\n",
    "        if not isinstance(factors, list):\n",
    "            raise ValueError(\"Expected input to be a list of sequences.\")\n",
    "\n",
    "        # Get the lengths of each sequence\n",
    "        lengths = [factor.shape[0] for factor in factors]\n",
    "\n",
    "        # Concatenate all sequences into a single tensor along the batch dimension\n",
    "        # The batch size will be the total number of sequences, each with its own sequence length\n",
    "        cat_fs = torch.cat(factors, dim=0).unsqueeze(1)  # Add batch dimension (for GRU)\n",
    "\n",
    "        # Pass the concatenated sequence through the GRU\n",
    "        gru_output, _ = self.gru(cat_fs)\n",
    "        gru_output = self.dropout(gru_output)\n",
    "        # Apply the linear transformation (fc) to the GRU output\n",
    "        rates = self.fc(gru_output)\n",
    "\n",
    "        # Remove the batch dimension and split the result back into the original list of sequences\n",
    "        rates = rates.squeeze(1)  # Remove the added batch dimension\n",
    "\n",
    "        # Split back into the original sequences based on the lengths\n",
    "        rates = list(torch.split(rates, lengths))  # Convert tuple to list here\n",
    "\n",
    "        # Return the list of output sequences\n",
    "        return rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentEncoder_LN(nn.Module):\n",
    "    def __init__(self, input_size=12, hidden_size=32, num_layers=1, device=None):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True, bias=False)\n",
    "        \n",
    "        # Initialize the hidden state for GRU\n",
    "        self.initial_hidden_state = nn.Parameter(torch.zeros(1, 1, hidden_size))  # unidirectional GRU\n",
    "        \n",
    "        # Add layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        # Input should be a PackedSequence\n",
    "        if not isinstance(input_, PackedSequence):\n",
    "            raise ValueError('Input must be a PackedSequence.')\n",
    "        \n",
    "        batch_size = input_.unsorted_indices.shape[0]  # Extract batch size from PackedSequence\n",
    "        initial_hidden_state = self.initial_hidden_state.repeat(1, batch_size, 1)\n",
    "        \n",
    "        # Forward pass through GRU\n",
    "        factors, _ = self.gru(input_, initial_hidden_state)\n",
    "        \n",
    "        # Return unpacked sequence\n",
    "        factors = unpack_sequence(factors)\n",
    "        factors[0] = self.layer_norm(factors[0])\n",
    "        \n",
    "        return factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lineplots(all_probs_array, y_val, y_labels, trial, shift, n_epochs, hidden_size, file_path, features, model_type, size=(18, 10), show_plot=True):\n",
    "    # Create a figure with subplots for each variable\n",
    "    fig, axs = plt.subplots(len(y_labels), 1, figsize=size,linewidth=0.01)\n",
    "    \n",
    "    if len(y_labels) == 1:\n",
    "        axs = [axs]  # Wrap the single Axes object in a list   \n",
    "    \n",
    "    for i, label in enumerate(y_labels):\n",
    "        ax = axs[i]\n",
    "        ax.plot(y_val[:, i], label='True', linewidth=2.5, color='blue')  # True values in blue\n",
    "        ax.plot(all_probs_array[:, i], label='Predicted', linewidth=2.5, color='orange')  # Predicted values in orange\n",
    "\n",
    "        # Set titles and labels\n",
    "        if i == 0:  # Only set title for the first plot\n",
    "            ax.set_title(f'({model_type} (packing)) (val trial: {trial}, lag {shift}, epochs: {n_epochs}, hidden states: {hidden_size})')\n",
    "            ax.legend()\n",
    "        \n",
    "        ax.set_xlabel('Time Step',labelpad=0.5)\n",
    "        ax.set_ylabel(label)\n",
    "        feature_names = \", \".join(features)\n",
    "        fig.text(.5, .00001, f\"Features: {feature_names}\", ha='center', fontsize=14)\n",
    "        \n",
    "        if file_path is not None:\n",
    "            plt.savefig(file_path)\n",
    "\n",
    "    # Show the plots if requested\n",
    "    if show_plot:\n",
    "        plt.tight_layout()  # Adjust spacing between subplots for better layout\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer, loss_fn, device=None):\n",
    "    epoch_loss = []\n",
    "    epoch_correct, epoch_total = 0, 0\n",
    "    \n",
    "    model.train()\n",
    "    for x, y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(*x)\n",
    "        \n",
    "        \n",
    "        loss = loss_fn(y_pred, y)\n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return np.mean(epoch_loss), y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dataloader, model, loss_fn, device=None):\n",
    "    epoch_loss = []\n",
    "    epoch_correct, epoch_total = 0, 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            y_pred = model(*x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            epoch_loss.append(loss.item())\n",
    "    \n",
    "    return np.mean(epoch_loss), y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.0, verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "            min_delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "            verbose (bool): If True, prints messages about early stopping progress.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = model.state_dict()  # Save the initial model state\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.best_model_state = model.state_dict()  # Update the best model state\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def restore_best_model(self, model):\n",
    "        \"\"\"\n",
    "        Restores the model state to the best state observed during training.\n",
    "        \"\"\"\n",
    "        if self.best_model_state is not None:\n",
    "            model.load_state_dict(self.best_model_state)\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class M2S_Loss3(nn.Module):\n",
    "    def __init__(self, criterion=None, pad_value=0.0):\n",
    "        super().__init__()\n",
    "        self.criterion = criterion if criterion is not None else nn.MSELoss(reduction='mean')\n",
    "        self.pad_value = pad_value  # Define the padding value (default to 0.0)\n",
    "\n",
    "    def forward(self, y_pred, y):\n",
    "        if isinstance(y_pred, list) and isinstance(y, list):\n",
    "            total_loss = 0\n",
    "            total_valid_sequences = 0\n",
    "\n",
    "            for i in range(len(y_pred)):\n",
    "                pred_seq = y_pred[i]\n",
    "                target_seq = y[i]\n",
    "\n",
    "                # Ensure both sequences have the same length\n",
    "                valid_length = min(pred_seq.shape[0], target_seq.shape[0])\n",
    "\n",
    "                # Slice sequences to valid lengths (no padding assumed here)\n",
    "                pred_seq = pred_seq[:valid_length]\n",
    "                target_seq = target_seq[:valid_length]\n",
    "\n",
    "                # Create a mask to ignore padded values (assuming padding is represented by pad_value)\n",
    "                mask = (target_seq != self.pad_value).float()\n",
    "\n",
    "                # Calculate the loss with the mask\n",
    "                loss = self.criterion(pred_seq * mask, target_seq * mask)  # Apply mask to ignore padding\n",
    "                total_loss += loss.sum()  # Sum loss over sequence\n",
    "                total_valid_sequences += valid_length  # Count valid sequence length\n",
    "            \n",
    "            # Return the mean loss over all valid elements\n",
    "            return total_loss / total_valid_sequences\n",
    "        else:\n",
    "            raise ValueError('y_pred and y should be lists of sequences.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_ES_C(\n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    model, \n",
    "    optimizer, \n",
    "    loss_fn, \n",
    "    num_epochs, \n",
    "    scheduler=None, \n",
    "    device=None, \n",
    "    verbose=False, \n",
    "    patience=20, \n",
    "    save_dir=\".\", \n",
    "    model_name=\"model\"\n",
    "    ):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    yPred_tr, yPred_val = [], []    \n",
    "\n",
    "    # Initialize early stopping\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=verbose)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training step\n",
    "        epoch_train_loss, yPred_tr = train(train_dataloader, model, optimizer, loss_fn, device)\n",
    "        \n",
    "        # Validation step\n",
    "        epoch_val_loss, yPred_val = validate(val_dataloader, model, loss_fn, device)\n",
    "        \n",
    "        # Scheduler step\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(epoch_train_loss)\n",
    "        \n",
    "        # Log the losses\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "\n",
    "        # Early stopping check\n",
    "        early_stopping(epoch_val_loss, model)\n",
    "\n",
    "        # Save model every 200 epochs\n",
    "        #if epoch % 200 == 0 and epoch != 0:\n",
    "           # checkpoint_path = f'{save_dir}/{model_name}_epoch_{epoch}_checkpoint.pth'\n",
    "            #torch.save(model.state_dict(), checkpoint_path)\n",
    "            #print(f\"Model checkpoint saved at epoch {epoch}: {checkpoint_path}\")\n",
    "\n",
    "        # Print training information\n",
    "        if verbose or epoch % 50 == 0 and epoch != 0:\n",
    "            print(f\"Epoch {epoch}, train loss: {epoch_train_loss}, val loss: {epoch_val_loss}\")\n",
    "        \n",
    "        # Break training if early stopping criteria are met\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered. Restoring the best model state...\")\n",
    "            model = early_stopping.restore_best_model(model)\n",
    "            break\n",
    "\n",
    "    # Final restoration of the best model\n",
    "    model = early_stopping.restore_best_model(model)\n",
    "\n",
    "\n",
    "    # Save the final best model after all epochs\n",
    "    best_model_path = f'{save_dir}/{model_name}_best_model.pth'\n",
    "    torch.save(model.state_dict(), best_model_path)\n",
    "    print(f\"Final best model saved: {best_model_path}\")\n",
    "    return train_losses, val_losses, yPred_tr, yPred_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your data\n",
    "data = ''  # Path to your dataset\n",
    "df = pd.read_csv(data)\n",
    "\n",
    "# Filter trials (if necessary)\n",
    "keep_trials = [1, 3, 5, 6, 10, 12, 13, 14, 15, 16, 20, 22, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 37, 39, 40, 42, 43, 44, 45, 61]\n",
    "if 'trial' in df.columns:\n",
    "    df = df[df['trial'].isin(keep_trials)]\n",
    "\n",
    "# Define fixed values for each scenario\n",
    "scenarios = {\n",
    "    1: {\n",
    "        'oDistReach_1': 0.039192008, 'oDistReach_2': 0.159473814, 'oDistReach_3': 0.244949143, 'oDistReach_4': 0.40015473,\n",
    "        'oAngHGFeed_1': 53.20439684, 'oAngHGFeed_2': 21.58116759, 'oAngHGFeed_3': 14.74049229, 'oAngHGFeed_4': 19.9144559\n",
    "    },\n",
    "    2: {\n",
    "        'oDistReach_1': 0.168446136, 'oDistReach_2': 0.020607723, 'oDistReach_3': 0.161489225, 'oDistReach_4': 0.268309158,\n",
    "        'oAngHGFeed_1': 29.60866461, 'oAngHGFeed_2': 19.4084781, 'oAngHGFeed_3': 59.38005131, 'oAngHGFeed_4': 68.25748778\n",
    "    },\n",
    "    3: {\n",
    "        'oDistReach_1': 0.28621023,  'oDistReach_2': 0.168894108, 'oDistReach_3': 0.030250559, 'oDistReach_4': 0.146278534,\n",
    "        'oAngHGFeed_1': 21.38281388, 'oAngHGFeed_2': 16.46802007, 'oAngHGFeed_3': 54.0270024,  'oAngHGFeed_4': 72.08987075\n",
    "    },\n",
    "    4: {\n",
    "        'oDistReach_1': 0.420576716, 'oDistReach_2': 0.274890588, 'oDistReach_3': 0.153489252, 'oDistReach_4': 0.040491932,\n",
    "        'oAngHGFeed_1': 41.21796036, 'oAngHGFeed_2': 17.32473936, 'oAngHGFeed_3': 23.17393409, 'oAngHGFeed_4': 43.38455563\n",
    "    },\n",
    "}\n",
    "\n",
    "# Apply each scenario and save to a new file\n",
    "for scenario, values in scenarios.items():\n",
    "    df_scenario = df.copy()\n",
    "    for column, value in values.items():\n",
    "        df_scenario[column] = value\n",
    "\n",
    "    # Save the scenario to a separate file\n",
    "    output_file = f'dyadic_scenario_{scenario}_dataset.csv' # Output file name for the scenario\n",
    "    df_scenario.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Scenarios saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler, RobustScaler, StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler, Normalizer, QuantileTransformer, PowerTransformer\n",
    "lag_values = np.arange(1, 26)\n",
    "folds = [[1, 16, 25, 30, 37]]\n",
    "drop_trials = [4, 7, 21, 46, 47, 48, 54, 58, 17, 8, 52, 56, 17]\n",
    "keep_trials = [1, 3, 5, 6, 10, 12, 13, 14, 15, 16, 20, 22, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 37, 39, 40, 42, 43, 44, 45, 61]\n",
    "test_trials = []\n",
    "start = perf_counter()\n",
    "\n",
    "feature_set1 = [\"aAngHGFeed_1_lagged\",\"aAngHGFeed_2_lagged\",\"aAngHGFeed_3_lagged\",\"aAngHGFeed_4_lagged\"]\n",
    "feature_set2 = [\"aAngHG_oHead_lagged\",\"aAngHG_oMid_lagged\",\"aAngHG_oReach_lagged\"]\n",
    "feature_set3 = [\"aDistReach_1_lagged\",\"aDistReach_2_lagged\",\"aDistReach_3_lagged\",\"aDistReach_4_lagged\"]\n",
    "feature_set4 = [\"oDistReach_1_lagged\",\"oDistReach_2_lagged\",\"oDistReach_3_lagged\",\"oDistReach_4_lagged\"]\n",
    "feature_set5 = [\"oAngHGFeed_1_lagged\",\"oAngHGFeed_2_lagged\",\"oAngHGFeed_3_lagged\",\"oAngHGFeed_4_lagged\"]\n",
    "standard_scaler = MinMaxScaler()\n",
    "feature_sets = [feature_set1, feature_set2, feature_set3, feature_set4, feature_set5]\n",
    "hidden_values = [24,32,64,128]\n",
    "lag_values = np.arange(1,26)\n",
    "# Iterate over different lag values\n",
    "for h in hidden_values:\n",
    "    for s in lag_values:\n",
    "        # Define the feature sets based on model type\n",
    "        model_type = 'full'\n",
    "        hidden_size = h\n",
    "        set_value = [1, 16, 25, 30, 37]\n",
    "        dec = 'gd'\n",
    "        n_epochs = 2000\n",
    "        shift = s\n",
    "        model_name = f'{model_type}_h{hidden_size}_e{n_epochs}_v{set_value}_s{shift}'\n",
    "        labels = ['aDistReach_1', 'aDistReach_2', 'aDistReach_3', 'aDistReach_4']\n",
    "        save_dir = f'/kaggle/working/{model_type}_h{hidden_size}'\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "    \n",
    "        # Prepare data file and load the dataset\n",
    "        data = '/kaggle/input/df-filtered-bin/df_binned.csv'\n",
    "    \n",
    "        # Columns for shifting\n",
    "        columns_to_shift = ['aAngHGFeed_1', 'aAngHGFeed_2', 'aAngHGFeed_3', 'aAngHGFeed_4',\n",
    "       'aAngHG_oHead', 'aAngHG_oMid', 'aAngHG_oReach', 'aDistReach_1',\n",
    "       'aDistReach_2', 'aDistReach_3', 'aDistReach_4', 'oDistReach_1',\n",
    "       'oDistReach_2', 'oDistReach_3', 'oDistReach_4', 'oAngHGFeed_1',\n",
    "       'oAngHGFeed_2', 'oAngHGFeed_3', 'oAngHGFeed_4']\n",
    "        feature_sets = [feature_set1,feature_set2,feature_set3,feature_set4,feature_set5]\n",
    "        df = pd.read_csv(data)\n",
    "        df = df[df['trial'].isin(keep_trials)]\n",
    "        \n",
    "    \n",
    "        # Apply log transformation to the distance variables for better prediction near zero\n",
    "        df[['oDistReach_1', 'oDistReach_2', 'oDistReach_3', 'oDistReach_4']] = np.log(df[['oDistReach_1', 'oDistReach_2', 'oDistReach_3', 'oDistReach_4']] + 1e-5)\n",
    "        df[['aDistReach_1', 'aDistReach_2', 'aDistReach_3', 'aDistReach_4']] = np.log(df[['aDistReach_1', 'aDistReach_2', 'aDistReach_3', 'aDistReach_4']] + 1e-5)\n",
    "        # Handle angle variables (scale them differently)\n",
    "    \n",
    "        angle_columns = [\"oAngHGFeed_1\",\"oAngHGFeed_2\",\"oAngHGFeed_3\",\"oAngHGFeed_4\"]\n",
    "        df[angle_columns] = MinMaxScaler().fit_transform(df[angle_columns])\n",
    "    \n",
    "            # Scale angle variables using MaxAbsScaler for proportional consistency\n",
    "            \n",
    "        lag_word = '_lagged'\n",
    "        input_size = len(columns_to_shift)\n",
    "        output_size = len(labels)\n",
    "        # Now create lagged columns\n",
    "        shifted_df = pd.DataFrame()\n",
    "        for trial_value in df['trial'].unique():\n",
    "            trial_df = df[df['trial'] == trial_value].copy()\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_lagged'\n",
    "                trial_df[new_col_name] = trial_df[col].shift(-s)\n",
    "            trial_df = trial_df.dropna()\n",
    "            shifted_df = pd.concat([shifted_df, trial_df], ignore_index=True)\n",
    "    \n",
    "        # Train and validation set preparation\n",
    "        features = shifted_df.columns[shifted_df.columns.str.contains('_lagged')]\n",
    "        train_set = shifted_df[~shifted_df['trial'].isin(set_value)]\n",
    "        train_dataset = TrialDataset_seq(train_set, feature_sets, y=labels, seq_len=1)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=5, shuffle=True, collate_fn=collate_packed_sequences)\n",
    "    \n",
    "        val_set = shifted_df[shifted_df['trial'].isin(set_value)]\n",
    "        val_dataset = TrialDataset_seq(val_set, feature_sets, y=labels, seq_len=1)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=len(set_value), shuffle=False, collate_fn=collate_packed_sequences)\n",
    "    \n",
    "                # Initialize encoders for each feature set\n",
    "        encoders = [ExperimentEncoder_LN(input_size=len(feature_set), hidden_size=hidden_size) for feature_set in feature_sets]\n",
    "        combined_encoder = CombinedEncoder(encoders)\n",
    "        if dec == 'ld':\n",
    "            readout = NonlinearDecoder(hidden_size * len(feature_sets), output_size)\n",
    "        if dec == 'gd':\n",
    "            readout = GRUDecoder(input_size=hidden_size * len(feature_sets), hidden_size=hidden_size, output_size=output_size)\n",
    "        model = CoreAndReadout(combined_encoder, readout)\n",
    "        model = model.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=20, threshold=0.0001)\n",
    "        loss = nn.MSELoss()\n",
    "        loss_fn = M2S_Loss3(loss)\n",
    "    \n",
    "    \n",
    "        models_dir = save_dir + \"/models/\"\n",
    "        if not os.path.exists(models_dir):\n",
    "            os.makedirs(models_dir)\n",
    "        train_losses, val_losses, yPred_tr, yPred_val = run_training_ES_C(train_dataloader, val_dataloader, model, optimizer, loss_fn,\n",
    "                                                                          n_epochs, scheduler=scheduler, device=device, verbose=False, patience=50, save_dir=models_dir, model_name=model_name)\n",
    "    \n",
    "        fig, ax = plt.subplots(figsize=(18, 4))\n",
    "        ax.plot((train_losses), label='Train Loss', color='blue')\n",
    "        ax.plot((val_losses), label='Validation Loss', color='orange')\n",
    "        ax.set_title((f'Learning curve ({model_type} - packed): (val trial: {set_value}, lag {shift}, epochs: {n_epochs}, hidden states: {hidden_size})'))\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.legend(loc='upper right')\n",
    "        ax.grid()\n",
    "        plot_name = f'{model_type}_{set_value}_e{n_epochs}_h{hidden_size}_l{shift}_LC.eps'\n",
    "        plt.savefig(plot_name, format='eps')\n",
    "        lc_folder =  save_dir + \"/LC/\"\n",
    "        if not os.path.exists(lc_folder):\n",
    "            os.makedirs(lc_folder)\n",
    "        lc_dir = os.path.join(lc_folder, plot_name)\n",
    "        plt.savefig(lc_dir)\n",
    "        plt.show()\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, outputs) in enumerate(val_dataloader):\n",
    "                inputs = [input_.to(device) for input_ in inputs]\n",
    "                outputs = [output.to(device) for output in outputs]\n",
    "                y_pred_batch = model(*inputs)\n",
    "        \n",
    "                for i in np.arange(len(y_pred_batch)):\n",
    "                    print(f'Trial: {set_value[i]}')\n",
    "        \n",
    "                    prediction = y_pred_batch[i]\n",
    "                    true_value = outputs[i]\n",
    "        \n",
    "                    # Ensure both prediction and true_value have the same length\n",
    "                    valid_length = min(prediction.shape[0], true_value.shape[0])\n",
    "                    prediction = prediction[:valid_length]  # Cut prediction to valid length\n",
    "                    true_value = true_value[:valid_length]  # Cut true_value to valid length\n",
    "        \n",
    "                    # Convert to numpy for plotting\n",
    "                    prediction_np = prediction.detach().cpu().numpy()\n",
    "                    true_value_np = true_value.detach().cpu().numpy()\n",
    "        \n",
    "                    # Save the plot with a descriptive filename\n",
    "                    plot_filename = f'{model_type}_v{set_value[i]}_e{n_epochs}_h{hidden_size}_l{shift}.png'\n",
    "                    plot_dir = os.path.join(save_dir, plot_filename)\n",
    "                    # Call the plot function with the adjusted predictions and true values\n",
    "                    create_lineplots(\n",
    "                        all_probs_array=np.exp(prediction_np),  # Assuming you want to plot the exp of values\n",
    "                        y_val=np.exp(true_value_np),  # Apply exp to the true values as well\n",
    "                        y_labels=labels,\n",
    "                        trial=set_value[i],\n",
    "                        shift=shift,\n",
    "                        n_epochs=n_epochs,\n",
    "                        hidden_size=hidden_size,\n",
    "                        file_path=None,\n",
    "                        features=features,\n",
    "                        size=(20, 12),\n",
    "                        show_plot=True,\n",
    "                        model_type=model_type\n",
    "                    )\n",
    "\n",
    "\n",
    "end = perf_counter()\n",
    "print(f\"Time taken to execute code : {end-start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping trials shorter than 3.5 sec and these with too many missing values\n",
    "drop_trials = [4, 7, 21, 46, 47, 48, 54, 58, 17, 8, 52, 56, 17]\n",
    "keep_trials = [1, 3, 5, 6, 10, 12, 13, 14, 15, 16, 20, 22, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 37, 39, 40, 42, 43, 44, 45, 61]\n",
    "len(keep_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "angle_scaler = MinMaxScaler()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "drop_trials = [4, 7, 21, 46, 47, 48, 54, 58, 17, 8, 52, 56, 17]\n",
    "keep_trials = [1, 3, 5, 6, 10, 12, 13, 14, 15, 16, 20, 22, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 37, 39, 40, 42, 43, 44, 45, 61]\n",
    "folds = [1, 16, 25, 30, 37]\n",
    "models = ['solo','dyadic','full']\n",
    "lag_values = np.arange(1, 26)\n",
    "standard_scaler = MinMaxScaler()\n",
    "decoders = ['gd','ld']\n",
    "\n",
    "\n",
    "# Initialize a list to store ALL results outside all loops\n",
    "all_results = []\n",
    "hidden_values = [16]\n",
    "for d in decoders:\n",
    "    for m in models:\n",
    "        for shift in lag_values:\n",
    "            for val in folds:\n",
    "                # Set up parameters\n",
    "                dec = d\n",
    "                model_type = m\n",
    "                hidden_size = 16\n",
    "                n_epochs = 2000\n",
    "                model_name = f'{model_type}_h{hidden_size}_e{n_epochs}_v[1, 16, 25, 30, 37]_s{shift}'\n",
    "                model_path = f'/{model_type}_h{hidden_size}_{dec}_1enc/models/{model_name}_best_model.pth'\n",
    "                predictions_dir = f'/{model_type}_h{hidden_size}_{dec}_1enc/predictions'\n",
    "                data = f'datasets/df_binned_filtered.csv'\n",
    "\n",
    "                # Define labels and columns\n",
    "                labels = ['aDistReach_1', 'aDistReach_2', 'aDistReach_3', 'aDistReach_4']\n",
    "                if model_type == 'full':\n",
    "                    columns_to_shift = ['aAngHGFeed_1', 'aAngHGFeed_2', 'aAngHGFeed_3', 'aAngHGFeed_4',\n",
    "                                    'aAngHG_oHead', 'aAngHG_oMid', 'aAngHG_oReach', 'aDistReach_1',\n",
    "                                    'aDistReach_2', 'aDistReach_3', 'aDistReach_4', 'oDistReach_1',\n",
    "                                    'oDistReach_2', 'oDistReach_3', 'oDistReach_4', 'oAngHGFeed_1',\n",
    "                                    'oAngHGFeed_2', 'oAngHGFeed_3', 'oAngHGFeed_4']\n",
    "                elif model_type == 'dyadic':\n",
    "                    columns_to_shift = ['aDistReach_1', 'aDistReach_2', 'aDistReach_3', 'aDistReach_4', 'oDistReach_1',\n",
    "                                    'oDistReach_2', 'oDistReach_3', 'oDistReach_4']\n",
    "                else:\n",
    "                    columns_to_shift = ['aDistReach_1', 'aDistReach_2', 'aDistReach_3', 'aDistReach_4']\n",
    "\n",
    "                # Load and prepare data\n",
    "                lag_word = '_lagged'\n",
    "                input_size = len(columns_to_shift)\n",
    "                output_size = len(labels)\n",
    "                df = pd.read_csv(data)\n",
    "                df = df[df['trial'].isin(keep_trials)]\n",
    "                if model_type == 'full' or model_type == 'dyadic':\n",
    "                    df[['oDistReach_1', 'oDistReach_2', 'oDistReach_3', 'oDistReach_4']] = np.log(df[['oDistReach_1', 'oDistReach_2', 'oDistReach_3', 'oDistReach_4']] + 1e-5)\n",
    "                df[['aDistReach_1', 'aDistReach_2', 'aDistReach_3', 'aDistReach_4']] = np.log(df[['aDistReach_1', 'aDistReach_2', 'aDistReach_3', 'aDistReach_4']] + 1e-5)\n",
    "\n",
    "                # Handle angle variables\n",
    "                if model_type == 'full':\n",
    "                    angle_columns = ['aAngHGFeed_1', 'aAngHGFeed_2', 'aAngHGFeed_3', 'aAngHGFeed_4',\n",
    "                                'aAngHG_oHead', 'aAngHG_oMid', 'aAngHG_oReach',\n",
    "                                'oAngHGFeed_1', 'oAngHGFeed_2', 'oAngHGFeed_3', 'oAngHGFeed_4']\n",
    "                    df[angle_columns] = angle_scaler.fit_transform(df[angle_columns])\n",
    "\n",
    "                # Apply lag shifting\n",
    "                shifted_df = pd.DataFrame()\n",
    "                for trial_value in df['trial'].unique():\n",
    "                    trial_df = df[df['trial'] == trial_value].copy()\n",
    "                    for col in columns_to_shift:\n",
    "                        trial_df[f'{col}{lag_word}'] = trial_df[col].shift(-shift)\n",
    "                    trial_df = trial_df.dropna()\n",
    "                    shifted_df = pd.concat([shifted_df, trial_df], ignore_index=True)\n",
    "\n",
    "                # Extract features and prepare datasets\n",
    "                features = shifted_df.columns[shifted_df.columns.str.contains(lag_word)]\n",
    "                val_set = shifted_df[shifted_df['trial'] == val]\n",
    "                \n",
    "                val_dataset = TrialDataset_seq(val_set, X=features, y=labels, seq_len=1)\n",
    "                val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=collate_packed_sequences)\n",
    "\n",
    "                # Load model\n",
    "                core = ExperimentEncoder_LN(input_size, hidden_size, num_layers=1)\n",
    "                if dec == 'ld':\n",
    "                    readout = NonlinearDecoder(hidden_size, output_size)\n",
    "                if dec == 'gd':\n",
    "                    readout = GRUDecoder(hidden_size,output_size)\n",
    "                model = CoreAndReadout(core, readout)\n",
    "                model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\n",
    "                model = model.to(device)\n",
    "                model.eval()\n",
    "\n",
    "                # Define loss functions\n",
    "                loss_fn = M2S_Loss3(nn.MSELoss())\n",
    "\n",
    "                # Evaluate\n",
    "                with torch.no_grad():\n",
    "                    for batch_idx, (inputs, outputs) in enumerate(val_dataloader):\n",
    "                        inputs = inputs.to(device)\n",
    "                        outputs = [output.to(device) for output in outputs]\n",
    "                        y_pred_batch = model(inputs)\n",
    "\n",
    "                        for i in range(len(y_pred_batch)):\n",
    "                            prediction = y_pred_batch[i]\n",
    "                            prediction_np = np.exp(prediction.detach().cpu().numpy())\n",
    "                            true_value_np = np.exp(outputs[i].detach().cpu().numpy())\n",
    "\n",
    "                            r2_h = r2_score(true_value_np, prediction_np)\n",
    "                            mse_h = np.mean((true_value_np - prediction_np) ** 2)\n",
    "                            ade_h = np.mean(np.linalg.norm(prediction_np - true_value_np, axis=1))\n",
    "                            mape_h = np.mean(np.abs((true_value_np - prediction_np) / true_value_np)) * 100\n",
    "                            rmse_h = np.sqrt(mse_h)\n",
    "\n",
    "                            # Store results with decoder type\n",
    "                            all_results.append({\n",
    "                                'model_type':  model_type + \"_\" + dec + \"_1enc\",\n",
    "                                'shift': shift,\n",
    "                                'val': val,\n",
    "                                'hidden_size': hidden_size,\n",
    "                                'r2': r2_h,\n",
    "                                'mse': mse_h,\n",
    "                                'ade': ade_h,\n",
    "                                'mape': mape_h,\n",
    "                                'rmse': rmse_h\n",
    "                            })\n",
    "\n",
    "                            # Save predictions to file\n",
    "                            prediction_df = pd.DataFrame({\n",
    "                                **{f'true_{label}': true_value_np[:, idx] for idx, label in enumerate(labels)},\n",
    "                                **{f'pred_{label}': prediction_np[:, idx] for idx, label in enumerate(labels)}\n",
    "                            })\n",
    "                            prediction_file_path = f'{predictions_dir}/trial{val}_shift{shift}_hidden{hidden_size}.csv'\n",
    "                            if not os.path.exists(predictions_dir):\n",
    "                                os.makedirs(predictions_dir)\n",
    "                            prediction_df.to_csv(prediction_file_path, index=False)\n",
    "\n",
    "        print(f\"Completed evaluating {model_type} model with {dec} decoder\")\n",
    "\n",
    "# Save all results to a single CSV after all evaluations are complete\n",
    "final_results_df = pd.DataFrame(all_results)\n",
    "final_results_path = 'results.csv'\n",
    "final_results_df.to_csv(final_results_path, index=False)\n",
    "print(f\"Evaluation complete. All results saved to {final_results_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
